<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CS8850 - Advanced Machine Learning</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 20px;
    }
    h1, h2, h3 {
      color: #2c3e50;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: 20px;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 8px;
      text-align: left;
    }
    th {
      background-color: #f2f2f2;
    }
    ul {
      padding-left: 20px;
    }
    a {
      color: #3498db;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .center {
      text-align: center;
    }
    .instructor-details, .ta-details {
      display: flex;
      justify-content: space-between;
      margin-bottom: 20px;
      text-align: center;
    }
    .instructor-details div, .ta-details div {
      width: 48%;
    }
  </style>
</head>
<body>

  <h1 class="center">CS8850 - Advanced Machine Learning - Spring 2025</h1>
  <h2 class="center">Georgia State University, Atlanta, GA</h2>

  <div class="instructor-details">
    <div>
      <h3>Instructor</h3>
      <p><strong>Name:</strong> Prof. Sergey Plis</p>
      <p><strong>Email:</strong> <a href="mailto:splis@gsu.edu">splis@gsu.edu</a></p>
      <p><strong>Office Hours:</strong> TBA</p>
      <p><strong>Office:</strong> 55 Park Place, Room 1821</p>
    </div>
    <div>
      <h3>Graduate Teaching Assistant</h3>
      <p><strong>Name:</strong> Mansoor Ahmed</p>
      <p><strong>Email:</strong> <a href="mailto:mahmed76@gsu.edu">mahmed76@gsu.edu</a></p>
      <p><strong>Office Hours:</strong> TBA</p>
      <p><strong>Office:</strong> TBA</p>
    </div>
  </div>

  <h2>Course Description</h2>
  <p>
    Machine learning studies algorithms that build models from data for subsequent use in prediction, inference, and decision-making tasks. Although an active field for the last 60 years, the current demand as well as trust in machine learning exploded as increasingly more data become available and the problems needed to be addressed become literally impossible to program directly. In this advanced course, we will cover essential algorithms, concepts, and principles of machine learning. Along with the traditional exposition, we will learn how these principles are currently being revisited thanks to the recent discoveries in the field.
  </p>

  <h2>Prerequisites</h2>
  <ul>
    <li><strong>Programming:</strong> Proficiency in at least one programming language, preferably Python, as homework will be given with the expectation of Python knowledge.</li>
    <li><strong>Mathematics:</strong> Basic knowledge of linear algebra, optimization theory, probability, and statistics. The class is taught under the assumption that you have taken some machine learning, statistics, calculus, probability, and optimization theory classes and are ready for advanced material.</li>
  </ul>

  <h2>Reading List</h2>
  <p>
    I will use parts of the following textbooks, accompanied by mandatory research papers and optional reading material that I may recommend to supplement the lectures.
  </p>

  <ul>
    <li>
      Bishop, C. M. 
      <a href="https://www.cs.uoi.gr/~arly/courses/ml/tmp/Bishop_book.pdf">Pattern Recognition and Machine Learning</a>.
      Springer; 2006.
    </li>
    <li>
      Shalev-Shwartz, S., & Ben-David, S. 
      <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a>.
      Cambridge University Press; 2014.
    </li>
    <li>
      MacKay, D. J. 
      <a href="http://www.inference.org.uk/itila/book.html">Information Theory, Inference and Learning Algorithms</a>.
      Cambridge University Press; 2003.
    </li>
    <li>
      Goodfellow, I., Bengio, Y., & Courville, A. 
      <a href="http://alvarestech.com/temp/deep/Deep%20Learning%20by%20Ian%20Goodfellow,%20Yoshua%20Bengio,%20Aaron%20Courville%20(z-lib.org).pdf">Deep Learning</a>.
      MIT Press; 2016.
    </li>
    <li>
      Provost, F., & Fawcett, T. 
      <a href="https://www.advisory21.com.mt/wp-content/uploads/2023/05/Data-Science-for-Business.pdf">Data Science for Business: What You Need to Know About Data Mining and Data-Analytic Thinking</a>.
      2013.
    </li>
    <li>
      Hastie, T., Tibshirani, R., & Friedman, J. 
      <a href="https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf">The Elements of Statistical Learning</a>.
    </li>
    <li>
      Hamming, R. 
      <a href="https://worrydream.com/refs/Hamming_1997_-_The_Art_of_Doing_Science_and_Engineering.pdf">The Art of Doing Science and Engineering: Learning to Learn</a>.
    </li>
  </ul>

  <h2>Course Schedule and Resources</h2>
  <p>Below is the detailed course schedule with subtopics, lecture videos (with timestamps), and additional reading materials.</p>

  <table>
    <thead>
      <tr>
        <th>S.No</th>
        <th>Topic</th>
        <th>Subtopics</th>
        <th>Lecture Videos</th>
        <th>Reading References</th>
      </tr>
    </thead>
    <tbody>


<!-- Week 1 -->
<tr>
  <td>1</td>
  <td>Mathematical Foundations -- Refresher</td>
  <td>
    <ul>
      <li>Linear Algebra</li>
      <li>Applied Probability</li>
      <li>Differential Equations and Calculus</li>
      <li>Optimization Theory</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://uchicago.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%229a8246fb-a85d-4683-9652-ac41012e590c%22">UChicago -- Math for ML</a></li>
      <li><a href="https://www.3blue1brown.com/topics/linear-algebra">3Blue1Brown</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://mml-book.github.io/book/mml-book.pdf">Mathematics for Machine Learning by Soon Ong</a></li>
      <li><strong>Books:</strong> Goodfellow -- Chapt. 2 (pg. 29-45), Bishop -- Chapt. 2 (pg. 67-93)</li>
      <li><strong>Other:</strong> Exceptional lectures and books on LA by <a href="https://math.mit.edu/~gs/">Prof. Gilbert Strang</a></li>
    </ul>
  </td>
</tr>

<!-- Week 1 -->
<tr>
  <td>1</td>
  <td>Foundations of Learning</td>
  <td>
    <ul>
      <li>Formal Learning Model</li>
      <li>Generalization and Overfitting</li>
      <li>Empirical Risk Minimization (ERM)</li>
      <li>ERM with Inductive Bias</li>
      <li>Bound the probability of error -- confidence and accuracy</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/yqUG88kTByI?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Formalizing the Problem of Learning (24:19)</a></li>
      <li><a href="https://youtu.be/GCqoI5J7iRg">Inductive Bias (12:03)</a></li>
      <li><a href="https://youtu.be/B2lJ6ZQgWjU?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Can We Bound the Probability of Error? (25:56)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://pages.cs.wisc.edu/~kandasamy/courses/24fall-cs861/lectures/lec0102_pac_learning_intro.pdf">ERM -- UWisconsin</a>, <a href="https://webdav.tuebingen.mpg.de/is-class-2/Lecture1.pdf">UTuebingen</a>, <a href="https://www.eecis.udel.edu/~xwu/class/ELEG867/Lecture2.pdf">UDelware</a></li>
      <li><strong>Books:</strong> Shai Chapt. 2 (pg. 33-41)</li>
      <li><strong>Slides:</strong> <a href="https://neuroneural.net/AML/cs8850_02_erm.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 2 -->
<tr>
  <td>2</td>
  <td>PAC Learning</td>
  <td>
    <ul>
      <li>PAC Learning Framework</li>
      <li>VC Dimension</li>
      <li>Sample Complexity</li>
      <li>Learnability Conditions</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/x4x7Wg_kpsA?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Main Definitions (13:52)</a></li>
      <li><a href="https://youtu.be/KDaBG4L3P8A?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Agnostic PAC Learning (53:35)</a></li>
      <li><a href="https://youtu.be/s3DRQHr9SVI?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Learning via Uniform Convergence (10:15)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://www.cs.cornell.edu/courses/cs6781/2020sp/lectures/03-pac1.pdf">Cornell</a>, <a href="https://www.cs.princeton.edu/courses/archive/spring08/cos511/scribe_notes/0211.pdf">Princeton</a>, <a href="https://www.cis.upenn.edu/~danroth/Teaching/CS446-17/LectureNotesNew/colt/main.pdf">UPenn</a></li>
      <li><strong>Books:</strong> Shai Chapt. 3-4 (pg. 43-58), Shai Chapt. 6 (pg. 67-78)</li>
      <li><strong>Slides:</strong> <a href="https://engineering.purdue.edu/ChanGroup/ECE595/files/Lecture24_pac.pdf">Purdue</a>, <a href="https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture28-pac.pdf">CMU</a>, <a href="https://neuroneural.net/AML/cs8850_03_pac.html">Lecture Slides</a></li>
      <li><strong>Videos:</strong> <a href="https://www.youtube.com/watch?v=qOMOYM0WCzU">Ali -- UWaterloo</a></li>
    </ul>
  </td>
</tr>

<!-- Week 3 -->
<tr>
  <td>3</td>
  <td>Linear Learning Models</td>
  <td>
    <ul>
      <li>Linear decision boundary -- binary classifier</li>
      <li>Perceptron algorithm -- batch & stochastic</li>
      <li>Proof of convergence</li>
      <li>Inseparable case</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/G5Adbaj54Ew?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Linear Decision Boundary (34:10)</a></li>
      <li><a href="https://youtu.be/MAvLeK4cbUI?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Perceptron (37:10)</a></li>
      <li><a href="https://youtu.be/rAbf1h16dZo?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Perceptron Extensions (14:09)</a></li>
      <li><a href="https://youtu.be/619G2Mgnlpw?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Linear Classifier for Linearly Non-Separable Classes (8:59)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://course.ccs.neu.edu/cs6140sp15/2_GD_REG_pton_NN/lecture_notes/lectureNotes_Perceptron.pdf">Northeastern</a>, <a href="https://www.comp.nus.edu.sg/~scarlett/CS5339_notes/01-Perceptron_Notes.pdf">NUS</a></li>
      <li><strong>Slides:</strong> <a href="https://neuroneural.net/AML/cs8850_05_linear.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 4 -->
<tr>
  <td>4</td>
  <td>Principal Component Analysis</td>
  <td>
    <ul>
      <li>Linear Regression (LR), Least Means Squares (LMS)</li>
      <li>Spectral theorem, similarity transform, eigenvectors, diagonalization, spectral factorization</li>
      <li>PCA -- quadratic forms, multivariate Gaussian density, Isodensity surfaces, Principal Axes Theorem</li>
      <li>Eigenvectors and Eigenvalues</li>
      <li>Covariance Matrix -- Diagonalizing the Covariance Matrix, KL-transform, ex. bivariate case</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/D3aelYmwfL8?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Linear Regression (39:24)</a></li>
      <li><a href="https://youtu.be/Ow3iluZkHJs?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Linear Algebra Micro Refresher (2:04)</a></li>
      <li><a href="https://youtu.be/ZNRQFAx28WY?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Spectral Theorem (25:54)</a></li>
      <li><a href="https://youtu.be/EXDMzm3_e78?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Principal Component Analysis (22:29)</a></li>
      <li><a href="https://youtu.be/15tPLTsDeyg?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Demonstration (17:38)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> 
        <ul>
          <li>LR: <a href="https://courses.grainger.illinois.edu/ece420/fa2017/AdaptiveSignalProcessing.pdf">UIUC</a>, <a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf">Stanford -- Part 1</a></li>
          <li>PCA: <a href="https://www.math.uchicago.edu/~may/VIGRE/VIGRE2009/REUPapers/Mei.pdf">UChicago</a></li>
        </ul>
      </li>
      <li><strong>Slides:</strong> <a href="https://www.cs.toronto.edu/~rahulgk/courses/csc311_f23/lectures/lec08.pdf">UToronto</a>, <a href="https://neuroneural.net/AML/cs8850_06_pca.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 5 -->
<tr>
  <td>5</td>
  <td>Curse of Dimensionality</td>
  <td>
    <ul>
      <li>The curse of dimensionality</li>
      <li>Volume in high-dimensional space -- Stirling approximation, Hypersphere Volume</li>
      <li>Ex: Gaussian distributions in high dimensional space</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/jcFKqAbExeE?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Curse of Dimensionality (1:16:27)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf">CMU</a>, <a href="https://www.cs.princeton.edu/courses/archive/fall15/cos521/lecnotes/lec12.pdf">Princeton</a></li>
      <li><strong>Books:</strong> Bishop Chapt. 1 (pg. 33-37), Hastie Chapt. 2 (pg. 22-23), Hamming Chapt. 9 (pg. 58-66)</li>
      <li><strong>Slides:</strong> <a href="https://neuroneural.net/AML/cs8850_07_curse.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 6 -->
<tr>
  <td>6</td>
  <td>Bayesian Decision Theory</td>
  <td>
    <ul>
      <li>Review probability distributions, Random variables, joint and marginal probabilities</li>
      <li>Bayes Theorem, Prior and Posterior Distributions</li>
      <li>Decision Rules -- Continuous and Discrete Features</li>
      <li>Bayesian vs Frequentist Approaches</li>
      <li>General Bayesian Decision Theory</li>
      <li>Maximum A Posteriori (MAP) -- conjugate priors</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/se3cKtL30i0?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Bayesian Decision Theory (56:47)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://www.cs.toronto.edu/~ucacsjp/ProbLearning.pdf">UToronto -- Probability refresher</a>, <a href="https://www.stat.cmu.edu/~larry/=sml/Bayes.pdf">CMU (pg. 1-27)</a>, <a href="https://www.stat.cmu.edu/~brian/463-663/week09/Chapter%2003.pdf">CMU</a></li>
      <li><strong>Books:</strong> Duda Chapt. 1-2 (pg. 3-37), Bishop Chapt. 1 (pg. 39-49)</li>
      <li><strong>Slides:</strong> <a href="https://neuroneural.net/AML/cs8850_08_bayes.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 7 -->
<tr>
  <td>7</td>
  <td>Parameter Estimation -- MLE</td>
  <td>
    <ul>
      <li>Maximum Likelihood Estimation (MLE) -- conditional independence, MLE for Bernoulli and Gaussian distributions, sample complexity & PAC learning</li>
      <li>MLE and KL-divergence -- Hartley's Information and Shannon's entropy, cross-entropy, KL-divergence minimization</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/pulSqfONQBs?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Independence (12:07)</a></li>
      <li><a href="https://youtu.be/XPwXf8N9Uq0">Maximum Likelihood Estimation (50:35)</a></li>
      <li><a href="https://youtu.be/j1ytTIYfdlY?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">MLE as KL-divergence Minimization (21:41)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://www.cs.cmu.edu/~10315/notes/10315_S24_Notes_MLE.pdf">CMU</a>, <a href="https://math.ou.edu/~cremling/teaching/lecturenotes/stat/ln3.pdf">UOklahoma</a>, <a href="https://web.stanford.edu/class/archive/cs/cs109/cs109.1206/lectureNotes/LN20_parameters_mle.pdf">Stanford</a></li>
      <li><strong>Books:</strong> Duda Chapt. 10 (pg. 107-117), Bishop Chapt. 2 (pg. 78-97), Goodfellow Chapt. 5 (pg. 128-131)</li>
      <li><strong>Blogs:</strong> <a href="https://leimao.github.io/blog/Cross-Entropy-KL-Divergence-MLE/">Cross-Entropy-KL-Divergence-MLE</a></li>
      <li><strong>Slides:</strong> <a href="https://neuroneural.net/AML/cs8850_09_MLE.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 7 -->
<tr>
  <td>7</td>
  <td>Parameter Estimation -- MAP & NB</td>
  <td>
    <ul>
      <li>Maximum A Posteriori (MAP) Estimation</li>
      <li>MLE vs. MAP</li>
      <li>MAP for binomial and multinomial distributions</li>
      <li>Bayes rule -- AIDS test example</li>
      <li>Naive Bayes classifier -- continuous and discrete features, text classification example</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/iACJJ4_AN4I?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">MAP Estimation (56:00)</a></li>
      <li><a href="https://youtu.be/QsAe2e7d9m0?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">The Na√Øve Bayes Classifier (37:09)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://www.cs.columbia.edu/~mcollins/em.pdf">Columbia</a></li>
      <li><strong>Slides:</strong> <a href="https://www.cs.cmu.edu/~aarti/Class/10315_Fall19/lecs/Lecture4.pdf">CMU 1</a>, <a href="https://www.cs.cmu.edu/~tom/10601_sp08/slides/recitation-mle-nb.pdf">CMU 2</a>, <a href="https://neuroneural.net/AML/cs8850_10_naive_bayes.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 8 -->
<tr>
  <td>8</td>
  <td>Logistic Regression</td>
  <td>
    <ul>
      <li>Naive Bayes recap -- Gaussian NB as a linear classifier, generative vs. discriminative classifiers</li>
      <li>Defining Logistic Regression -- Linear Fit to Log-Odds, softmax</li>
      <li>Solving Logistic Regression -- an alternative perspective on log odds, Logistic Sigmoid, MLE & Negative Log likelihood -- Taylor expansion, Newton-Raphson update for linear and logistic regression</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/SRG2CgyQ9TY?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">NB to LR (19:49)</a></li>
      <li><a href="https://youtu.be/kb3uUypVC8I?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Defining Logistic Regression (27:42)</a></li>
      <li><a href="https://youtu.be/eC58QL1BZCA?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Solving Logistic Regression (23:35)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf">Stanford</a>, <a href="https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf">CMU</a></li>
      <li><strong>Books:</strong> Bishop Chapt. 4 (pg. 205-210), Shai Chapt. 9 (pg. 126-128)</li>
      <li><strong>Slides:</strong> <a href="https://mkang.faculty.unlv.edu/teaching/CS489_689/09.Logistic%20Regression.pdf">UNevada</a>, <a href="https://neuroneural.net/AML/cs8850_11_logistic_regression.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 9 -->
<tr>
  <td>9</td>
  <td>Kernel Density Estimation</td>
  <td>
    <ul>
      <li>Density Estimation Basics -- Non-parametric density estimation, histogram-based, Parzen windows, smooth kernels</li>
      <li>Bandwidth Selection, Bias-variance tradeoff (digression)</li>
      <li>Multivariate density estimation, Product kernels, Unimodal and Bimodal distribution KDE</li>
      <li>Applications of KDE</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/kJBmvF9HqCc?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Non-parametric Density Estimation (1:13:33)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://faculty.washington.edu/yenchic/17Sp_403/Lec7-density.pdf">UWashington</a></li>
      <li><strong>Books:</strong> Duda Chapt. 4 (pg. 187-198)</li>
      <li><strong>Slides:</strong> <a href="https://cse.buffalo.edu/~jcorso/t/CSE555/files/lecture_nonprm.pdf">Buffalo</a>, <a href="https://people.engr.tamu.edu/rgutier/web_courses/cs790_w02/l10.pdf">Texas A&M</a>, <a href="https://neuroneural.net/AML/cs8850_12_kde_etc.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 10 -->
<tr>
  <td>10</td>
  <td>Support Vector Machines</td>
  <td>
    <ul>
      <li>Maximum Margin Classifier -- Bayes decision boundary, Restricted Bayes optimal classifier, Linear SVM Classifier, Linear SVM: primal formulation, problems and solutions</li>
      <li>Lagrange Duality -- Karush-Kuhn-Tucker (KKT) conditions, Quadratic programming</li>
      <li>Dual Formulation of SVM</li>
      <li>Kernel Tricks -- Mapping to Higher Dimensions, Mercer's theorem</li>
      <li>Soft Margin</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/0QM95dkRdW4?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Max Margin Classifier (35:53)</a></li>
      <li><a href="https://youtu.be/s_gJ0j_fEZU?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Lagrange Multipliers (32:45)</a></li>
      <li><a href="https://youtu.be/4eEuFI5FInY?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Dual Formulation of Linear SVM (10:34)</a></li>
      <li><a href="https://youtu.be/_5OK22gB2O8?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Kernel Trick and Soft Margin (27:28)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> <a href="https://www.cse.msu.edu/~cse802/S17/slides/Lec_17_Mar27_SVM.pdf">SVMs & KMs</a></li>
      <li><strong>Books:</strong> Shai Chapt. 15 (pg. 202-214), Bishop Chapt. 7 (pg. 325-344)</li>
      <li><strong>Slides:</strong> <a href="https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture06.pdf">SVR</a>, <a href="https://neuroneural.net/AML/cs8850_13_svm.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

<!-- Week 11 -->
<tr>
  <td>11</td>
  <td>Matrix Factorization</td>
  <td>
    <ul>
      <li>Singular Value Decomposition (SVD), Cocktail party problem</li>
      <li>Independent Component Analysis (ICA) -- Linear vs statistical independence</li>
      <ul>
        <li>Methods: projection pursuit, infomax (mutual information), and MLE</li>
        <li>FastICA</li>
      </ul>
      <li>Non-negative Matrix Factorization (NMF)</li>
      <li>Dictionary Learning</li>
      <li>Autoencoders</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/rkEJdK8nxsw?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Matrix Factorization (1:24:22)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Notes:</strong> 
        <ul>
          <li>NMF: <a href="https://people.math.ethz.ch/~nicolai/mv/notes4.pdf">ETH</a></li>
          <li>ICA: <a href="https://www.cs.jhu.edu/~ayuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf">UHelsinki</a></li>
          <li>DL: <a href="https://users.ece.utexas.edu/~sanghavi/courses/scribed_notes/Lecture_24_Scribe_Notes.pdf">UTexas</a></li>
        </ul>
      </li>
      <li><strong>Slides:</strong> <a href="https://angms.science/doc/NMF/nmf_0.pdf">ANGMS</a>, <a href="https://neuroneural.net/AML/cs8850_14_factorization.html">Lecture Slides</a></li>
    </ul>
  </td>
</tr>

      

<!-- Week 15 -->
<tr>
  <td>15</td>
  <td>Stochastic Gradient Descent (SGD)</td>
  <td>
    <ul>
      <li>Gradient Descent Basics</li>
      <li>Stochastic vs Batch Gradient Descent</li>
      <li>Learning Rate Scheduling</li>
      <li>Convergence Properties</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/FkfDlOnQVvQ?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Stochastic Gradient Descent (1:06:57)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_15_SGD.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Shai - Chapt. 14 (pg. 184-201)</li>
      <li><strong>Notes:</strong> <a href="https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture5.pdf">Cornell Notes</a>, <a href="https://nikolaimatni.github.io/courses/ese680-fall2019/scribe-notes/lecture16.pdf">Nikolai Notes</a></li>
    </ul>
  </td>
</tr>

<!-- Week 16 -->
<tr>
  <td>16</td>
  <td>k-means Clustering</td>
  <td>
    <ul>
      <li>Clustering Basics</li>
      <li>Hard k-means</li>
      <li>Soft k-means</li>
      <li>Gaussian Mixture Models (GMM)</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/cxcHmgYuXpE?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Clustering (6:05)</a></li>
      <li><a href="https://youtu.be/Q9yV86BrCII?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Gaussian Mixture Models (16:34)</a></li>
      <li><a href="https://youtu.be/meRPb0SbGxc?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">MLE Recap (4:20)</a></li>
      <li><a href="https://youtu.be/838SN9pqj7c?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Hard k-means Clustering (30:27)</a></li>
      <li><a href="https://youtu.be/SnO77_iy0sA?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Soft k-means Clustering (7:18)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_16_kmeans.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Bishop - Chapt. 9 (pg. 423-455), Duda - Chapt. 10 (pg. 617-619)</li>
      <li><strong>Notes:</strong> <a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes7a.pdf">Stanford Notes</a>, <a href="https://www2.stat.duke.edu/~rcs46/lectures_2017/10-unsupervise/10-kmeans_v2.pdf">Princeton Notes</a></li>
      <li><strong>Papers:</strong> <a href="https://web.mit.edu/6.435/www/Dempster77.pdf">MLE via EM</a></li>
    </ul>
  </td>
</tr>

<!-- Week 17 -->
<tr>
  <td>17</td>
  <td>Expectation Maximization (EM)</td>
  <td>
    <ul>
      <li>EM Algorithm</li>
      <li>Gaussian Mixture Models (GMM)</li>
      <li>Convergence Properties</li>
      <li>Applications of EM</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/J-3If9t0p_0?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Do We Even Need EM for GMM? (14:39)</a></li>
      <li><a href="https://youtu.be/Bn6L3rB9eiw?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">A "Hacky" GMM Estimation (15:17)</a></li>
      <li><a href="https://youtu.be/tWOO3g_BZ-Y?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">MLE via EM (38:28)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_17_EM.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Shai - Chapt. 24 (pg. 348-352), Bishop - Chapt. 9 (pg. 423-455)</li>
      <li><strong>Notes:</strong> <a href="https://users.cs.duke.edu/~cynthia/CourseNotes/GMMEMNotes.pdf">Duke Notes</a>, <a href="https://web.stanford.edu/~lmackey/stats306b/doc/stats306b-spring14-lecture2_scribed.pdf">Stanford Notes</a></li>
      <li><strong>Papers:</strong> <a href="https://web.mit.edu/6.435/www/Dempster77.pdf">Dempster et al. (1977)</a></li>
    </ul>
  </td>
</tr>

<!-- Week 18 -->
<tr>
  <td>18</td>
  <td>Automatic Differentiation</td>
  <td>
    <ul>
      <li>Forward Mode AD</li>
      <li>Reverse Mode AD</li>
      <li>Backpropagation</li>
      <li>Applications in Deep Learning</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/Ct7N2VOrglk?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Introduction (25:10)</a></li>
      <li><a href="https://youtu.be/gQ3IG6nt2TY?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Forward Mode AD (26:46)</a></li>
      <li><a href="https://youtu.be/xHXwJgA-HiQ?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">A Minute of Backprop (2:26)</a></li>
      <li><a href="https://youtu.be/rzkmkT-_LuY?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Reverse Mode AD (17:26)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_18_AD.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Bishop - Chapt. 5 (pg. 225-284), Shai - Chapt. 20 (pg. 268-282)</li>
      <li><strong>Notes:</strong> <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf">Toronto Notes</a>, <a href="https://arxiv.org/pdf/2110.06209">UCSC Notes</a></li>
      <li><strong>Papers:</strong> <a href="https://arxiv.org/abs/1904.02990v3">Forward Mode AD</a>, <a href="https://arxiv.org/pdf/1502.05767">AD in ML</a></li>
    </ul>
  </td>
</tr>

<!-- Week 19 -->
<tr>
  <td>19</td>
  <td>Nonlinear Embedding Approaches</td>
  <td>
    <ul>
      <li>Manifold Learning</li>
      <li>t-SNE</li>
      <li>UMAP</li>
      <li>Applications in Visualization</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/Bz1UCuX-ynM?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Manifold Learning (20:13)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_19_manifold.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Duda - Chapt. 10 (pg. 661-669)</li>
      <li><strong>Notes:</strong> <a href="https://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/slides/ManifoldLearning.pdf">CMU Notes</a>, <a href="https://www.stat.pitt.edu/sungkyu/course/2221Fall13/lec8_mds_combined.pdf">UPittsburg Notes</a></li>
      <li><strong>Papers:</strong> <a href="https://arxiv.org/pdf/1802.03426.pdf">t-SNE Paper</a></li>
    </ul>
  </td>
</tr>

<!-- Week 20 -->
<tr>
  <td>20</td>
  <td>Model Comparison I</td>
  <td>
    <ul>
      <li>Bias-Variance Tradeoff</li>
      <li>No Free Lunch Theorem</li>
      <li>Confusion Matrix</li>
      <li>Cross-Validation</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/QFGqk9iZfYk?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Bias-Variance Tradeoff (36:52)</a></li>
      <li><a href="https://youtu.be/eIxocDyv1q0?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">No Free Lunch Theorem (7:29)</a></li>
      <li><a href="https://youtu.be/4fE1-GM6yWs?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Problems with Accuracy (12:39)</a></li>
      <li><a href="https://youtu.be/5qmz7EpvC9U?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Confusion Matrix (25:15)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_20_model_comparison_1.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Shai - Chapt. 5 (pg. 61-78), Bishop - Chapt. 3 (pg. 145-170)</li>
      <li><strong>Notes:</strong> <a href="https://cs229.stanford.edu/summer2019/BiasVarianceAnalysis.pdf">Stanford Notes</a></li>
      <li><strong>Papers:</strong> <a href="https://arxiv.org/pdf/1812.11118">Bias-Variance Tradeoff</a>, <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=585893">No Free Lunch Theorem</a></li>
    </ul>
  </td>
</tr>

<!-- Week 21 -->
<tr>
  <td>21</td>
  <td>Model Comparison II</td>
  <td>
    <ul>
      <li>Cross-Validation and Hyperopt</li>
      <li>Expected Value Framework</li>
      <li>Visualizing Model Performance</li>
      <li>Receiver Operating Characteristics (ROC)</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/gyLWpT2hFq8?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Cross-Validation and Hyperopt (29:08)</a></li>
      <li><a href="https://youtu.be/iTriOxl3XIo?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Expected Value Framework (22:48)</a></li>
      <li><a href="https://youtu.be/eJE5w2iD1G8?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Visualizing Model Performance (31:02)</a></li>
      <li><a href="https://youtu.be/8xe9wzWBdyM?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Receiver Operating Characteristics (22:34)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_21_model_comparison_2.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Shai - Chapt. 11 (pg. 145-160), Bishop - Chapt. 7 (pg. 325-360)</li>
      <li><strong>Notes:</strong> <a href="https://scikit-learn.org/stable/modules/cross_validation.html">Sklearn Cross-Validation</a></li>
      <li><strong>Papers:</strong> <a href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Hyperparameter Optimization</a></li>
    </ul>
  </td>
</tr>

<!-- Week 22 -->
<tr>
  <td>22</td>
  <td>Model Calibration</td>
  <td>
    <ul>
      <li>Calibration Techniques</li>
      <li>Reliability Diagrams</li>
      <li>Platt Scaling</li>
      <li>Isotonic Regression</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/rRhKGNFyPhg?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">On Model Calibration (36:53)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_22_calibration.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Shai - Chapt. 12 (pg. 175-190), Bishop - Chapt. 4 (pg. 195-220)</li>
      <li><strong>Notes:</strong> <a href="https://arxiv.org/pdf/1706.04599">Calibration of Modern Neural Networks</a></li>
      <li><strong>Papers:</strong> <a href="https://cseweb.ucsd.edu/~elkan/kddbianca.pdf">Histogram Binning</a>, <a href="https://dl.acm.org/doi/10.1145/775047.775151">Isotonic Regression</a></li>
    </ul>
  </td>
</tr>

<!-- Week 23 -->
<tr>
  <td>23</td>
  <td>Convolutional Neural Networks (CNNs)</td>
  <td>
    <ul>
      <li>Building Blocks</li>
      <li>Skip Connections</li>
      <li>Fully Convolutional Networks</li>
      <li>Semantic Segmentation</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/qR4sdFPrqVg?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Building Blocks (39:22)</a></li>
      <li><a href="https://youtu.be/XHjnHTysyjg?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Skip Connections (38:46)</a></li>
      <li><a href="https://youtu.be/CGSwcJ8eJeM?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Fully Convolutional Networks (8:07)</a></li>
      <li><a href="https://youtu.be/S4HrJAFhkCA?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Semantic Segmentation with Twists (23:40)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_23_CNN.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Bishop - Chapt. 5 (pg. 225-284), Shai - Chapt. 20 (pg. 268-282)</li>
      <li><strong>Notes:</strong> <a href="https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">AlexNet Paper</a></li>
      <li><strong>Papers:</strong> <a href="https://arxiv.org/abs/1505.00387">Highway Networks</a>, <a href="https://arxiv.org/abs/1512.03385">ResNet</a></li>
    </ul>
  </td>
</tr>

<!-- Week 24 -->
<tr>
  <td>24</td>
  <td>Word Embedding</td>
  <td>
    <ul>
      <li>Bag of Words</li>
      <li>Word2Vec</li>
      <li>GloVe</li>
      <li>Applications in NLP</li>
    </ul>
  </td>
  <td>
    <ul>
      <li><a href="https://youtu.be/8tTxF8rjSZ8?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Introduction (10:35)</a></li>
      <li><a href="https://youtu.be/3GM8Azmx1bI?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Semantic Matrix (30:26)</a></li>
      <li><a href="https://youtu.be/QzT1dzQxOp0?list=PLWL87aJW5Y--YMBUgw4yn28_njbLMHINR">Word2Vec (54:22)</a></li>
    </ul>
  </td>
  <td>
    <ul>
      <li><strong>Slides:</strong> <a href="../../AML/cs8850_24_embedding.html">Lecture Slides</a></li>
      <li><strong>Books:</strong> Shai - Chapt. 21 (pg. 290-305), Bishop - Chapt. 12 (pg. 561-600)</li>
      <li><strong>Notes:</strong> <a href="https://arxiv.org/pdf/1301.3781.pdf">Word2Vec Paper</a></li>
      <li><strong>Papers:</strong> <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe Paper</a></li>
    </ul>
  </td>
</tr>

    </tbody>
  </table>

  <p style="text-align: center; color: red;" >I manage this page, errors and omissions accepted..</p>
    <li><strong>Tools and Libraries:</strong>
      <ul>
        <li><a href="https://scikit-learn.org/">Scikit-learn</a></li>
        <li><a href="https://www.tensorflow.org/">TensorFlow</a></li>
        <li><a href="https://pytorch.org/">PyTorch</a></li>
      </ul>
    </li>
  </ul>

</body>
</html>